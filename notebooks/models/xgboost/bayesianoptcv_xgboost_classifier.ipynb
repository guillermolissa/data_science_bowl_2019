{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST -  CLASSIFICATION - BAYESIAN OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn import datasets\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = 'data/train_test/'\n",
    "SEED = 47\n",
    "NITER = 20\n",
    "CV = 3\n",
    "SCORE = 'roc_auc'\n",
    "handlingnull = False\n",
    "NJOBS = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_pickle(DATAPATH+'X_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_pickle(DATAPATH+'y_train.pkl')['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148865, 1800)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# drop var with same values\n",
    "train_features = train_features.T.drop_duplicates().T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a DMatrix and handling Null values\n",
    "if handlingnull:\n",
    "    #train_features[np.isnan(train_features)] = -9999\n",
    "    xgtrain = xgb.DMatrix(train_features, train_labels, missing=-9999)\n",
    "else:\n",
    "    xgtrain = xgb.DMatrix(train_features.values, train_labels.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set general hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== General Parameters ======= #\n",
    "\n",
    "# Select the type of model to run at each iteration. gbtree or gblinear.\n",
    "booster = 'gbtree'\n",
    "\n",
    "\n",
    "# ======== Booster Parameters ======== # \n",
    "\n",
    "# Analogous to learning rate in GBM. \n",
    "# Typical final values to be used: 0.01-0.2\n",
    "eta = 0.01\n",
    "\n",
    "\n",
    "# Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "# A typical value to consider: sum(negative instances) / sum(positive instances)scale_pos_weight = 1\n",
    "scale_pos_weight = int((len(train_labels) - np.sum(train_labels.values))/np.sum(train_labels.values))\n",
    "\n",
    "\n",
    "# Learning Task Parameters\n",
    "\n",
    "# This defines the loss function to be minimized. \n",
    "# - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "# - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "#   you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "# - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "objective  = 'binary:logistic'\n",
    "\n",
    "\n",
    "# The metric to be used for validation data.\n",
    "# - rmse – root mean square error\n",
    "# - mae – mean absolute error\n",
    "# - logloss – negative log-likelihood\n",
    "# - error – Binary classification error rate (0.5 threshold)\n",
    "# - merror – Multiclass classification error rate\n",
    "# - mlogloss – Multiclass logloss\n",
    "# - auc: Area under the curve\n",
    "eval_metric = 'auc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[xgboost params](https://xgboost.readthedocs.io/en/latest/python/python_api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find num boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_rounds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=XGBClassifier(seed=SEED, booster=booster, objective=objective,  scale_pos_weight = scale_pos_weight, nthread=NJOBS)\n",
    "xgb_param = model.get_xgb_params()\n",
    "xgb_param['objective'] = objective\n",
    "\n",
    "if USEGPU:\n",
    "    xgb_param['tree_method'] = 'gpu_hist'\n",
    "    xgb_param['gpu_id'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round = 1000, nfold = CV, metrics = eval_metric, early_stopping_rounds = early_stopping_rounds, seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = cvresult.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of boosters:  265\n"
     ]
    }
   ],
   "source": [
    "print(\"Best number of boosters: \", n_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian optimization hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search space\n",
    "pds ={\n",
    "    # Defines the minimum sum of weights of all observations required in a child.\n",
    "    'min_child_weight':(14, 20),\n",
    "    \n",
    "    # A node is split only when the resulting split gives a positive reduction in the loss function. \n",
    "    # Gamma specifies the minimum loss reduction required to make a split.\n",
    "    'gamma':(0, 5),\n",
    "    \n",
    "    # Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    'subsample':(0.5, 1),\n",
    "    \n",
    "    # Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    'colsample_bytree':(0.1, 1),\n",
    "    \n",
    "    # The maximum depth of a tree\n",
    "    'max_depth': (5, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Objective Function\n",
    "def hyp_xgb(max_depth, subsample, colsample_bytree,min_child_weight, gamma):\n",
    "    params = {\n",
    "    'booster': booster,\n",
    "    'n_estimators': n_estimators,\n",
    "    'eta': eta,\n",
    "    'objective': objective,\n",
    "    'eval_metric':eval_metric, # Optional --> Use eval_metric if you want to stop evaluation based on eval_metric \n",
    "    'silent': 1\n",
    "     }\n",
    "    \n",
    "    \n",
    "    params['max_depth'] = int(round(max_depth))\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "\n",
    "    \n",
    "    \n",
    "    cv_result = xgb.cv(params, xgtrain, num_boost_round=1000,verbose_eval=False, early_stopping_rounds=10, metrics = eval_metric, maximize=True, nfold=CV)\n",
    "    return  max(cv_result['test-auc-mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(hyp_xgb, pds, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | max_depth | min_ch... | subsample |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8013  \u001b[0m | \u001b[0m 0.1687  \u001b[0m | \u001b[0m 3.9     \u001b[0m | \u001b[0m 7.192   \u001b[0m | \u001b[0m 18.34   \u001b[0m | \u001b[0m 0.989   \u001b[0m |\n"
     ]
    }
   ],
   "source": [
    "# Optimize\n",
    "optimizer.maximize(init_points=5, n_iter=NITER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.7,\n",
       " 'reg_lambda': 0.7,\n",
       " 'reg_alpha': 0.05,\n",
       " 'min_child_weight': 9,\n",
       " 'max_depth': 9,\n",
       " 'learning_rate': 0.01,\n",
       " 'gamma': 0.4,\n",
       " 'colsample_bytree': 0.6}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.max['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(SCORE,' : ', xgboost_rsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('output/models/bayesianoptcv_xgboost_classifier_bestparams_d' + str(datetime.now().date()) + '.npy', optimizer.max['params'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Xgboost (env)",
   "language": "python",
   "name": "xgboostenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
